# -*- coding: utf-8 -*-
"""BAT_conservation_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bd-x-fhhQ6w-iHGODnKwMGY7KPPTWYIX
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import csv
import sklearn
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm

from google.colab import files
import io

files = files.upload()

df = pd.read_csv(io.StringIO(files['Bat.Data.Current1.csv'].decode('latin-1')))

df.head()

"""Data Summary"""

df.describe(include='all')
# df.describe(exclude=[np.number])

"""Data Types"""

print(df.dtypes)

"""Check data in table"""

from tabulate import tabulate
print(tabulate(df, headers = 'keys', tablefmt = 'psql'))

"""Null values in total"""

df.isna().sum().sum()

"""Null values by variables"""

df.isna().sum()

df.info(verbose=True)

"""Filling conservation "Defficient Data" with 0"""

df['Conservation.category.numeric'] = df['Conservation.category.numeric'].fillna(0)

"""heatmap to check the correlation between the variables (body parts)

"""

import seaborn as sns
dff = df[['Head.body.lower.mm','Head.body.upper.mm','Head.body.avg.mm','Tail.lower.mm','Tail.upper.mm',
          'Tail.avg.mm','Ears.lower.mm','Ears.upper.mm','Ears.avg.mm','Hindfoot.lower.mm','Hindfoot.upper.mm',
          'Hindfoot.mm','Forearm.lower.mm','Forearm.upper.mm','Forearm.avg.mm','weight.lower.g','weight.upper.g',
          'weight.avg.g','I.Dental.Formula','C.Dental.Formula','P.Dental.Formula','M.Dental.Formula','Total.teeth']]
sns.heatmap(dff.corr(), annot=True)
sns.set(rc={'figure.figsize':(35,25)})

"""Heatmap of the average of the variables (given in excel) (body part)"""

import seaborn as sns
dff = df[['Head.body.avg.mm','Tail.avg.mm','Ears.avg.mm','Hindfoot.mm','Forearm.avg.mm','weight.avg.g','I.Dental.Formula','C.Dental.Formula','P.Dental.Formula','M.Dental.Formula','Total.teeth']]
sns.heatmap(dff.corr(), annot=True)
sns.set(rc={'figure.figsize':(15,15)})

"""Dropping single column"""

# df1 = df.loc[:, df.columns != 'Family']
# df1

"""Dropping multiple columns and adding into another dataframe"""

df1 = df.drop(['Family','Simple.Habitats','Island.mainland.classification.UNEDITED', 'Habitats','Total.teeth',
               'Island.mainland.classification', 'Conservation','Conservation.category','Primary.diet', 'Simple.Primary.diet','I.Dental.Formula','C.Dental.Formula','P.Dental.Formula','M.Dental.Formula'], axis=1)
df1

from tabulate import tabulate
print(tabulate(df1, headers = 'keys', tablefmt = 'psql'))

"""Check the correlation"""

df1.corr()

"""#Using MICE for handelling missing data"""

pip install miceforest

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import miceforest as mf
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

df1.corr()

df2 = df1.drop(['Species'], axis=1)
df2

from tabulate import tabulate
print(tabulate(df2, headers = 'keys', tablefmt = 'psql'))

lr = LinearRegression()
imp = IterativeImputer(estimator=lr, verbose=2, max_iter=30, tol=1e-10, imputation_order='roman')

print(df2.dtypes)

arr = imp.fit_transform(df2)

print(arr)

# # to estimate the experience missing value in first iteration (test set)
# print(imp.imputation_sequence_[1][2].coef_)
# print(imp.imputation_sequence_[1][2].intercept_)

# # to estimate the salary missing value in first iteration (test set)
# print(imp.imputation_sequence_[2][2].coef_)
# print(imp.imputation_sequence_[2][2].intercept_)

"""##Easy code!!"""

df2_imputed = df2.copy(deep=True)

df1_imputed = df1.copy(deep=True)

pip install fancyimpute

from fancyimpute import IterativeImputer

iterativeimputer = IterativeImputer()

df2_imputed.iloc[:,:] = iterativeimputer.fit_transform(df2_imputed)

df2_imputed.isna().sum()

df2_imputed



df3 = pd.merge(df2_imputed,df1[['Head.body.lower.mm','Species']],on='Head.body.lower.mm', how='left')

df3.head()

"""#Feature encoding"""

newdf = df3.dropna()

"""Label Encoding with sklearn"""

from sklearn import preprocessing
# label_encoder object knows how to understand word labels. 
label_encoder = preprocessing.LabelEncoder()
# Encode labels in column 'Country'. 
newdf2 = newdf
newdf2['Species'] = label_encoder.fit_transform(newdf['Species']) 
print(newdf2.head())

"""One hot encoding"""

# # importing one hot encoder 
# from sklearn.preprocessing import OneHotEncoder
# # creating one hot encoder object 
# onehotencoder = OneHotEncoder()
# #reshape the 1-D country array to 2-D as fit_transform expects 2-D and finally fit the object 
# X = onehotencoder.fit_transform(newdf2.Species.values.reshape(-1,1)).toarray()
# #To add this back into the original dataframe 
# dfOneHot = pd.DataFrame(X, columns = ["Species_"+str(int(i)) for i in range(newdf2.shape[1])]) 
# newdf3 = pd.concat([newdf2, dfOneHot], axis=1)
# #droping the country column 
# newdf3 = newdf3.drop(['Species'], axis=1) 
# #printing to verify 
# print(newdf3.head())

"""#AIC and BIC - Feature selection"""

from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import pandas as pd

y = newdf2['Species']

y.isna().sum()

X = newdf2.loc[:, newdf.columns != 'Species']

#add constant to predictor variables
X = sm.add_constant(X)

#fit regression model
model = sm.OLS(y, X).fit()

#view AIC of model
print(model.aic)

#view BIC of model
print(model.bic)

"""#ADABOOST"""

len(pd.unique(df3['Species']))

le=LabelEncoder()
y=le.fit_transform(Species)

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)











# Load libraries
from sklearn.ensemble import AdaBoostClassifier
from sklearn import datasets
# Import train_test_split function
from sklearn.model_selection import train_test_split
#Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics

# Load data

X = df.Head.body.lower.mm
y = df.Conservation.category.numeric